{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91MsGMTna_P9"
      },
      "source": [
        "# CBU5201 mini-project submission\n",
        "\n",
        "The mini-project has two separate components:\n",
        "\n",
        "\n",
        "1.   **Basic component** [6 marks]: Using the genki4k dataset, build a machine learning pipeline that takes as an input an image and predicts 1) whether the person in the image is similing or not 2) estimate the 3D head pose labels in the image.\n",
        "2.   **Advanced component** [10 marks]: Formulate your own machine learning problem and build a machine learning solution using the genki4k dataset (https://inc.ucsd.edu/mplab/398/). \n",
        "\n",
        "Your submission will consist of two Jupyter notebooks, one for the basic component and another one for advanced component. Please **name each notebook**:\n",
        "\n",
        "* CBU5201_miniproject_basic.ipynb\n",
        "* CBU5201_miniproject_advanced.ipynb\n",
        "\n",
        "then **zip and submit them toghether**.\n",
        "\n",
        "Each uploaded notebook should include: \n",
        "\n",
        "*   **Text cells**, describing concisely each step and results.\n",
        "*   **Code cells**, implementing each step.\n",
        "*   **Output cells**, i.e. the output from each code cell.\n",
        "\n",
        "and **should have the structure** indicated below. Notebooks might not be run, please make sure that the output cells are saved.\n",
        "\n",
        "How will we evaluate your submission?\n",
        "\n",
        "*   Conciseness in your writing (10%).\n",
        "*   Correctness in your methodology (30%).\n",
        "*   Correctness in your analysis and conclusions (30%).\n",
        "*   Completeness (10%).\n",
        "*   Originality (10%).\n",
        "*   Efforts to try something new (10%).\n",
        "\n",
        "Suggestion: Why don't you use **GitHub** to manage your project? GitHub can be used as a presentation card that showcases what you have done and gives evidence of your data science skills, knowledge and experience. \n",
        "\n",
        "Each notebook should be structured into the following 9 sections:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGn4ICrfqXZ"
      },
      "source": [
        "# 1 Author\n",
        "\n",
        "**Student Name**:  Yaoan Yang\n",
        "\n",
        "**Student ID**:  210976881\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38VQkcdKd6k"
      },
      "source": [
        "# 2 Problem formulation\n",
        "\n",
        "Describe the machine learning problem that you want to solve and explain what's interesting about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BwrtEdLDit"
      },
      "source": [
        "# 3 Machine Learning pipeline\n",
        "\n",
        "Describe your ML pipeline. Clearly identify its input and output, any intermediate stages (for instance, transformation -> models), and intermediate data moving from one stage to the next. It's up to you to decide which stages to include in your pipeline. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pre-Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating folder for training, test, validation, copying the neccesary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "base_dir = 'D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\dataset'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Directory with our training smile pictures\n",
        "train_smile_dir = os.path.join(train_dir, 'smile')\n",
        "\n",
        "# Directory with our training nosmile pictures\n",
        "train_nosmile_dir = os.path.join(train_dir, 'nosmile')\n",
        "\n",
        "# Directory with our validation smile pictures\n",
        "validation_smile_dir = os.path.join(validation_dir, 'smile')\n",
        "\n",
        "# Directory with our validation nosmile pictures\n",
        "validation_nosmile_dir = os.path.join(validation_dir, 'nosmile')\n",
        "\n",
        "# Directory with our validation smile pictures\n",
        "test_smile_dir = os.path.join(test_dir, 'smile')\n",
        "\n",
        "# Directory with our validation nosmile pictures\n",
        "test_nosmile_dir = os.path.join(test_dir, 'nosmile')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total training smile images: 1730\n",
            "total training nosmile images: 1470\n",
            "total validation smile images: 200\n",
            "total validation nosmile images: 190\n",
            "total test smile images: 232\n",
            "total test nosmile images: 177\n"
          ]
        }
      ],
      "source": [
        "print('total training smile images:', len(os.listdir(train_smile_dir)))\n",
        "print('total training nosmile images:', len(os.listdir(train_nosmile_dir)))\n",
        "print('total validation smile images:', len(os.listdir(validation_smile_dir)))\n",
        "print('total validation nosmile images:', len(os.listdir(validation_nosmile_dir)))\n",
        "print('total test smile images:', len(os.listdir(test_smile_dir)))\n",
        "print('total test nosmile images:', len(os.listdir(test_nosmile_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3200 images belonging to 2 classes.\n",
            "Found 390 images belonging to 2 classes.\n",
            "Found 409 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras as keras\n",
        "keras.__version__\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Building Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  (None, 74, 74, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPoolin  (None, 36, 36, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPoolin  (None, 17, 17, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  (None, 7, 7, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Compile and Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if sys.path[0] == \"\":\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 27s 261ms/step - loss: 0.6848 - acc: 0.5555 - val_loss: 0.6871 - val_acc: 0.5553\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 27s 273ms/step - loss: 0.6737 - acc: 0.5770 - val_loss: 0.6869 - val_acc: 0.5263\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 37s 366ms/step - loss: 0.6561 - acc: 0.6320 - val_loss: 0.6626 - val_acc: 0.6105\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 29s 290ms/step - loss: 0.6422 - acc: 0.6255 - val_loss: 0.6598 - val_acc: 0.5947\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 35s 346ms/step - loss: 0.6262 - acc: 0.6520 - val_loss: 0.6362 - val_acc: 0.6421\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 39s 395ms/step - loss: 0.6026 - acc: 0.6785 - val_loss: 0.6400 - val_acc: 0.6316\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 0.5681 - acc: 0.7175 - val_loss: 0.6349 - val_acc: 0.6447\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 34s 340ms/step - loss: 0.5347 - acc: 0.7360 - val_loss: 0.6424 - val_acc: 0.6237\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 32s 315ms/step - loss: 0.5113 - acc: 0.7565 - val_loss: 0.5420 - val_acc: 0.7289\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 30s 297ms/step - loss: 0.4791 - acc: 0.7770 - val_loss: 0.5515 - val_acc: 0.7237\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 29s 286ms/step - loss: 0.4527 - acc: 0.7955 - val_loss: 0.5394 - val_acc: 0.7421\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 32s 314ms/step - loss: 0.4228 - acc: 0.8145 - val_loss: 0.5114 - val_acc: 0.7474\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 31s 312ms/step - loss: 0.4074 - acc: 0.8235 - val_loss: 0.4990 - val_acc: 0.7658\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 31s 314ms/step - loss: 0.3741 - acc: 0.8390 - val_loss: 0.5307 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 31s 313ms/step - loss: 0.3329 - acc: 0.8625 - val_loss: 0.4860 - val_acc: 0.7605\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 32s 317ms/step - loss: 0.3323 - acc: 0.8570 - val_loss: 0.5478 - val_acc: 0.7526\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 52s 519ms/step - loss: 0.3147 - acc: 0.8710 - val_loss: 0.4603 - val_acc: 0.7789\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 79s 783ms/step - loss: 0.2832 - acc: 0.8820 - val_loss: 0.4987 - val_acc: 0.7605\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 79s 786ms/step - loss: 0.2760 - acc: 0.8930 - val_loss: 0.4720 - val_acc: 0.7711\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 75s 745ms/step - loss: 0.2768 - acc: 0.8870 - val_loss: 0.4561 - val_acc: 0.7947\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 75s 750ms/step - loss: 0.2397 - acc: 0.9160 - val_loss: 0.4601 - val_acc: 0.7895\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 75s 750ms/step - loss: 0.2218 - acc: 0.9195 - val_loss: 0.5077 - val_acc: 0.7658\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 74s 734ms/step - loss: 0.2174 - acc: 0.9225 - val_loss: 0.4682 - val_acc: 0.7868\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 74s 741ms/step - loss: 0.1958 - acc: 0.9275 - val_loss: 0.5232 - val_acc: 0.7974\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 73s 728ms/step - loss: 0.1874 - acc: 0.9315 - val_loss: 0.5249 - val_acc: 0.7921\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 73s 732ms/step - loss: 0.1601 - acc: 0.9405 - val_loss: 0.5713 - val_acc: 0.7553\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 99s 984ms/step - loss: 0.1491 - acc: 0.9485 - val_loss: 0.6211 - val_acc: 0.7658\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 63s 637ms/step - loss: 0.1297 - acc: 0.9595 - val_loss: 0.5491 - val_acc: 0.7868\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 74s 739ms/step - loss: 0.1201 - acc: 0.9600 - val_loss: 0.6157 - val_acc: 0.7737\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 71s 712ms/step - loss: 0.1051 - acc: 0.9645 - val_loss: 0.5484 - val_acc: 0.7921\n"
          ]
        }
      ],
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=390//20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Saving result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\basic_v1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**One Photo Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 118ms/step\n",
            "[[0.9822432]]\n",
            "smile\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[0.0582096]]\n",
            "nosmile\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "model = keras.models.load_model('D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\basic_v1.h5')\n",
        "#smile case\n",
        "img_path='D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\dataset\\\\test\\\\smile\\\\file1931.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = keras.utils.img_to_array(img)/255.0\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "prediction =model.predict(img_tensor)  \n",
        "print(prediction)\n",
        "if prediction[0][0]>0.5:\n",
        "    result='smile'\n",
        "else:\n",
        "    result='nosmile'\n",
        "print(result)\n",
        "#non-smile case\n",
        "img_path='D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\dataset\\\\test\\\\nosmile\\\\file3824.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = keras.utils.img_to_array(img)/255.0\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "prediction =model.predict(img_tensor)  \n",
        "print(prediction)\n",
        "if prediction[0][0]>0.5:\n",
        "    result='smile'\n",
        "else:\n",
        "    result='nosmile'\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  import sys\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.517204 - acc: 0.785000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "score = model.evaluate_generator(\n",
        "  test_generator,\n",
        "  steps=409//20,\n",
        "  max_queue_size=10,\n",
        "  workers=1,\n",
        "  use_multiprocessing=False,\n",
        "  verbose=0)\n",
        "print(\"loss: %.6f - acc: %.6f\" % (score[0], score[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nDXnzYLLH6"
      },
      "source": [
        "# 4 Transformation stage\n",
        "\n",
        "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F5_kI95LuZ2"
      },
      "source": [
        "# 5 Modelling\n",
        "\n",
        "Describe the ML model(s) that you will build. Explain why you have chosen them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPTSuaB9L2jU"
      },
      "source": [
        "# 6 Methodology\n",
        "\n",
        "Describe how you will train and validate your models, how model performance is assesssed (i.e. accuracy, confusion matrix, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQPxztuL9AW"
      },
      "source": [
        "# 7 Dataset\n",
        "\n",
        "Describe the dataset that you will use to create your models and validate them. If you need to preprocess it, do it here. Include visualisations too. You can visualise raw data samples or extracted features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qf7GN1aeXJI"
      },
      "source": [
        "# 8 Results\n",
        "\n",
        "Carry out your experiments here, explain your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSrJCR_cekPO"
      },
      "source": [
        "# 9 Conclusions\n",
        "\n",
        "Your conclusions, improvements, etc should go here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee6ef56facda7503055c4941e2c2083c4bcc9ecb08a66ac58f56d3b05ea5e5fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
