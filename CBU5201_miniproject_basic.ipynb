{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91MsGMTna_P9"
      },
      "source": [
        "# CBU5201 mini-project submission\n",
        "\n",
        "The mini-project has two separate components:\n",
        "\n",
        "\n",
        "1.   **Basic component** [6 marks]: Using the genki4k dataset, build a machine learning pipeline that takes as an input an image and predicts 1) whether the person in the image is similing or not 2) estimate the 3D head pose labels in the image.\n",
        "2.   **Advanced component** [10 marks]: Formulate your own machine learning problem and build a machine learning solution using the genki4k dataset (https://inc.ucsd.edu/mplab/398/). \n",
        "\n",
        "Your submission will consist of two Jupyter notebooks, one for the basic component and another one for advanced component. Please **name each notebook**:\n",
        "\n",
        "* CBU5201_miniproject_basic.ipynb\n",
        "* CBU5201_miniproject_advanced.ipynb\n",
        "\n",
        "then **zip and submit them toghether**.\n",
        "\n",
        "Each uploaded notebook should include: \n",
        "\n",
        "*   **Text cells**, describing concisely each step and results.\n",
        "*   **Code cells**, implementing each step.\n",
        "*   **Output cells**, i.e. the output from each code cell.\n",
        "\n",
        "and **should have the structure** indicated below. Notebooks might not be run, please make sure that the output cells are saved.\n",
        "\n",
        "How will we evaluate your submission?\n",
        "\n",
        "*   Conciseness in your writing (10%).\n",
        "*   Correctness in your methodology (30%).\n",
        "*   Correctness in your analysis and conclusions (30%).\n",
        "*   Completeness (10%).\n",
        "*   Originality (10%).\n",
        "*   Efforts to try something new (10%).\n",
        "\n",
        "Suggestion: Why don't you use **GitHub** to manage your project? GitHub can be used as a presentation card that showcases what you have done and gives evidence of your data science skills, knowledge and experience. \n",
        "\n",
        "Each notebook should be structured into the following 9 sections:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaGn4ICrfqXZ"
      },
      "source": [
        "# 1 Author\n",
        "\n",
        "**Student Name**:  Yaoan Yang\n",
        "\n",
        "**Student ID**:  210976881\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38VQkcdKd6k"
      },
      "source": [
        "# 2 Problem formulation\n",
        "\n",
        "Describe the machine learning problem that you want to solve and explain what's interesting about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BwrtEdLDit"
      },
      "source": [
        "# 3 Machine Learning pipeline\n",
        "\n",
        "Describe your ML pipeline. Clearly identify its input and output, any intermediate stages (for instance, transformation -> models), and intermediate data moving from one stage to the next. It's up to you to decide which stages to include in your pipeline. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pre-Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating folder for training, test, validation, copying the neccesary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "base_dir = 'D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\dataset'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Directory with our training smile pictures\n",
        "train_smile_dir = os.path.join(train_dir, 'smile')\n",
        "\n",
        "# Directory with our training nosmile pictures\n",
        "train_nosmile_dir = os.path.join(train_dir, 'nosmile')\n",
        "\n",
        "# Directory with our validation smile pictures\n",
        "validation_smile_dir = os.path.join(validation_dir, 'smile')\n",
        "\n",
        "# Directory with our validation nosmile pictures\n",
        "validation_nosmile_dir = os.path.join(validation_dir, 'nosmile')\n",
        "\n",
        "# Directory with our validation smile pictures\n",
        "test_smile_dir = os.path.join(test_dir, 'smile')\n",
        "\n",
        "# Directory with our validation nosmile pictures\n",
        "test_nosmile_dir = os.path.join(test_dir, 'nosmile')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total training smile images: 1730\n",
            "total training nosmile images: 1470\n",
            "total validation smile images: 200\n",
            "total validation nosmile images: 190\n",
            "total test smile images: 232\n",
            "total test nosmile images: 177\n"
          ]
        }
      ],
      "source": [
        "print('total training smile images:', len(os.listdir(train_smile_dir)))\n",
        "print('total training nosmile images:', len(os.listdir(train_nosmile_dir)))\n",
        "print('total validation smile images:', len(os.listdir(validation_smile_dir)))\n",
        "print('total validation nosmile images:', len(os.listdir(validation_nosmile_dir)))\n",
        "print('total test smile images:', len(os.listdir(test_smile_dir)))\n",
        "print('total test nosmile images:', len(os.listdir(test_nosmile_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3200 images belonging to 2 classes.\n",
            "Found 390 images belonging to 2 classes.\n",
            "Found 409 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras as keras\n",
        "keras.__version__\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized to 150x150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Building Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Compile and Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n",
            "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if sys.path[0] == \"\":\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.6842 - acc: 0.5655WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
            "100/100 [==============================] - 29s 270ms/step - loss: 0.6842 - acc: 0.5655 - val_loss: 0.6842 - val_acc: 0.5590\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 25s 252ms/step - loss: 0.6633 - acc: 0.6015\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.6487 - acc: 0.6310\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 0.6209 - acc: 0.6600\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 29s 288ms/step - loss: 0.5829 - acc: 0.7030\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 31s 309ms/step - loss: 0.5501 - acc: 0.7410\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 31s 304ms/step - loss: 0.5340 - acc: 0.7580\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 30s 303ms/step - loss: 0.5011 - acc: 0.7595\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 28s 275ms/step - loss: 0.4578 - acc: 0.7840\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 28s 283ms/step - loss: 0.4319 - acc: 0.8065\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 29s 289ms/step - loss: 0.3971 - acc: 0.8230\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.3811 - acc: 0.8440\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 0.3504 - acc: 0.8575\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 30s 301ms/step - loss: 0.3320 - acc: 0.8645\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 29s 293ms/step - loss: 0.3129 - acc: 0.8740\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 30s 301ms/step - loss: 0.3007 - acc: 0.8795\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 0.2634 - acc: 0.8995\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.2500 - acc: 0.9025\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 32s 314ms/step - loss: 0.2269 - acc: 0.9100\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 0.2126 - acc: 0.9235\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 0.1966 - acc: 0.9250\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 28s 285ms/step - loss: 0.1887 - acc: 0.9315\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 31s 307ms/step - loss: 0.1664 - acc: 0.9405\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 31s 306ms/step - loss: 0.1530 - acc: 0.9450\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 30s 303ms/step - loss: 0.1488 - acc: 0.9460\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 58s 578ms/step - loss: 0.1262 - acc: 0.9615\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 39s 393ms/step - loss: 0.1101 - acc: 0.9655\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 32s 314ms/step - loss: 0.0991 - acc: 0.9685\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 34s 337ms/step - loss: 0.0934 - acc: 0.9680\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 31s 306ms/step - loss: 0.0764 - acc: 0.9750\n"
          ]
        }
      ],
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Validation and Training result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\smile_and_nosmile.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20872\\2446334803.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\smile_and_nosmile.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**One Photo Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 366ms/step\n",
            "[[0.7292042]]\n",
            "smile\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "[[0.1454728]]\n",
            "nosmile\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "model = keras.models.load_model('D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\smile_and_nosmile.h5')\n",
        "#smile case\n",
        "img_path='D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\dataset\\\\test\\\\smile\\\\file1931.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = keras.utils.img_to_array(img)/255.0\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "prediction =model.predict(img_tensor)  \n",
        "print(prediction)\n",
        "if prediction[0][0]>0.5:\n",
        "    result='smile'\n",
        "else:\n",
        "    result='nosmile'\n",
        "print(result)\n",
        "#non-smile case\n",
        "img_path='D:\\\\desktop\\\\bupt\\\\ML\\\\Mini_Project\\\\dataset\\\\test\\\\nosmile\\\\file3824.jpg'\n",
        "img = keras.utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = keras.utils.img_to_array(img)/255.0\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "prediction =model.predict(img_tensor)  \n",
        "print(prediction)\n",
        "if prediction[0][0]>0.5:\n",
        "    result='smile'\n",
        "else:\n",
        "    result='nosmile'\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - ETA: 0s - loss: 0.4711 - acc: 0.9000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 111ms/step - loss: 0.4711 - acc: 0.9000\n",
            "loss: 0.471140 - acc: 0.900000\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "def evaluate_model(model, generator, nBatches):\n",
        "    score = model.evaluate_generator(generator=generator,  # Generator yielding tuples\n",
        "                                     steps=math.ceil(generator.samples / nBatches),\n",
        "                                     # number of steps (batches of samples) to yield from generator before stopping\n",
        "                                     max_queue_size=10,  # maximum size for the generator queue\n",
        "                                     workers=1,\n",
        "                                     # maximum number of processes to spin up when using process based threading\n",
        "                                     use_multiprocessing=False,  # whether to use process-based threading\n",
        "                                     verbose=1)\n",
        "    print(\"loss: %.6f - acc: %.6f\" % (score[0], score[1]))\n",
        "evaluate_model(model, test_generator, nBatches=816)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nDXnzYLLH6"
      },
      "source": [
        "# 4 Transformation stage\n",
        "\n",
        "Describe any transformations, such as feature extraction. Identify input and output. Explain why you have chosen this transformation stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F5_kI95LuZ2"
      },
      "source": [
        "# 5 Modelling\n",
        "\n",
        "Describe the ML model(s) that you will build. Explain why you have chosen them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPTSuaB9L2jU"
      },
      "source": [
        "# 6 Methodology\n",
        "\n",
        "Describe how you will train and validate your models, how model performance is assesssed (i.e. accuracy, confusion matrix, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZQPxztuL9AW"
      },
      "source": [
        "# 7 Dataset\n",
        "\n",
        "Describe the dataset that you will use to create your models and validate them. If you need to preprocess it, do it here. Include visualisations too. You can visualise raw data samples or extracted features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qf7GN1aeXJI"
      },
      "source": [
        "# 8 Results\n",
        "\n",
        "Carry out your experiments here, explain your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSrJCR_cekPO"
      },
      "source": [
        "# 9 Conclusions\n",
        "\n",
        "Your conclusions, improvements, etc should go here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('tensorflow')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee6ef56facda7503055c4941e2c2083c4bcc9ecb08a66ac58f56d3b05ea5e5fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
